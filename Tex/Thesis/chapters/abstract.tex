
\begin{abstract}

The aim of this work is to propose a meta-algorithm for automatic classification in the presence of discrete binary classes. Supervised classification at a fundamental level can be defined as the ability to extract rules that discriminate one class from the other. This is done on the basis of training data whose class membership is known with the ultimate objective of classifying new data whose class mapping is unknown. Classifier learning in the presence of overlapping class distributions is a challenging problem in machine learning. Overlapping classes are described by the presence of ambiguous areas in the feature space with a high density of points belonging to both classes. This often occurs in real-world datasets, one such example is numeric data denoting properties of particle decays derived from high-energy accelerators like the \gls{LHCb} at CERN. 
 
A significant body of research targeting the class overlap problem use ensemble classifiers to boost the performance of standard algorithms by using them iteratively in multiple stages or using multiple copies of the same model on different subsets of the input training data. The former is called \textit{boosting} and the latter is called \textit{bagging}. The algorithm proposed in this thesis targets a popular and challenging classification problem in high energy physics - that of improving the statistical significance of the Higgs discovery. The underlying dataset used to train the algorithm is experimental data built from the official ATLAS full-detector simulation with Higgs events (signal) mixed with different background events (background) that closely mimic the statistical properties of the signal generating class overlap. The algorithm proposed is a variant of the classical boosted decision tree which is known to be one of the most successful analysis techniques in experimental physics. The algorithm utilises a unified framework that combines two meta learning techniques - bagging and boosting. The results shows that this combination only works in the presence of a randomization trick in the base learners. The performance of the algorithm is mainly assessed on the basis of a physics inspired significance metric called the \textit{Approximate Median Significance} ($\sigma$). We also show how the algorithm fares compared to the leading machine learning solutions proposed using this dataset.  

\end{abstract}